{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR DeepSpeech Examples\n",
    "\n",
    "This notebook demonstrates how to use the DeepSpeech estimator in ART as well as how to use the ASR imperceptible attack with the estimator.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from deepspeech_pytorch.loader.data_loader import load_audio\n",
    "\n",
    "from art.estimators.speech_recognition.pytorch_deep_speech import PyTorchDeepSpeech\n",
    "from art.attacks.evasion.imperceptible_asr.imperceptible_asr_pytorch import ImperceptibleASRPytorch\n",
    "from art.config import ART_DATA_PATH\n",
    "from art.utils import get_file\n",
    "\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to download data\n",
    "data_dir = os.path.join(ART_DATA_PATH, \"deepspeech_audio\")\n",
    "current_dir = %pwd\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Download audio data\n",
    "#get_file('librispeech.py', 'https://raw.githubusercontent.com/SeanNaren/deepspeech.pytorch/master/data/librispeech.py', path=data_dir)\n",
    "\n",
    "#%cd $data_dir\n",
    "#!python librispeech.py --files-to-use test-clean.tar.gz\n",
    "#%cd $current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model and Data Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The deepspeech estimator\n",
    "speech_recognizer = PyTorchDeepSpeech(pretrained_model=\"librispeech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_waveform(waveform, title=\"\", sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Display waveform plot and audio play UI.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.plot(waveform)\n",
    "    ipd.display(ipd.Audio(waveform, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = dict([(speech_recognizer.model.labels[i], i) for i in range(len(speech_recognizer.model.labels))])\n",
    "def parse_transcript(path):\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        transcript = f.read().replace('\\n', '')\n",
    "    result = list(filter(None, [labels_map.get(x) for x in list(transcript)]))\n",
    "    return transcript, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with Some Audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A long audio\n",
    "x1 = load_audio(os.path.join(data_dir, \"LibriSpeech_dataset/test_clean/wav/1089-134686-0000.wav\"))\n",
    "label1, encoded_label1 = parse_transcript(os.path.join(data_dir, \"LibriSpeech_dataset/test_clean/txt/1089-134686-0000.txt\"))\n",
    "print(\"Encoded label: \", encoded_label1)\n",
    "print(\"Groundtrue label: \", label1)\n",
    "display_waveform(x1, title=\"Long Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A short audio\n",
    "x2 = load_audio(os.path.join(data_dir, \"LibriSpeech_dataset/test_clean/wav/1089-134691-0003.wav\"))\n",
    "label2, encoded_label2 = parse_transcript(os.path.join(data_dir, \"LibriSpeech_dataset/test_clean/txt/1089-134691-0003.txt\"))\n",
    "print(\"Encoded label: \", encoded_label2)\n",
    "print(\"Groundtrue label: \", label2)\n",
    "display_waveform(x2, title=\"Short Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another short audio\n",
    "x3 = load_audio(os.path.join(data_dir, \"LibriSpeech_dataset/test_clean/wav/1089-134691-0018.wav\"))\n",
    "label3, encoded_label3 = parse_transcript(os.path.join(data_dir, \"LibriSpeech_dataset/test_clean/txt/1089-134691-0018.txt\"))\n",
    "print(\"Encoded label: \", encoded_label3)\n",
    "print(\"Groundtrue label: \", label3)\n",
    "display_waveform(x3, title=\"Short Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Estimator Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Transcription Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = speech_recognizer.predict(np.array([x1]), transcription_output=True)\n",
    "print(\"Groundtrue label: \", label1)\n",
    "print(\"Predicted  label: \", pred1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = speech_recognizer.predict(np.array([x2]), transcription_output=True)\n",
    "print(\"Groundtrue label: \", label2)\n",
    "print(\"Predicted  label: \", pred2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = speech_recognizer.predict(np.array([x3]), transcription_output=True)\n",
    "print(\"Groundtrue label: \", label3)\n",
    "print(\"Predicted  label: \", pred3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([x1, x2, x3])\n",
    "pred_all = speech_recognizer.predict(x, transcription_output=True)\n",
    "print(\"Predicted  labels: \", pred_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASR Attack on the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_max_length = int(np.max([len(x1), len(x2), len(x3)]))\n",
    "\n",
    "# Create attack\n",
    "asr_attack = ImperceptibleASRPytorch(\n",
    "    estimator=speech_recognizer,\n",
    "    initial_eps=0.0005,\n",
    "    max_iter_1st_stage=250,\n",
    "    max_iter_2nd_stage=50,\n",
    "    learning_rate_1st_stage=0.000001,\n",
    "    learning_rate_2nd_stage=0.0000001,\n",
    "    optimizer_1st_stage=torch.optim.SGD,\n",
    "    optimizer_2nd_stage=torch.optim.SGD,\n",
    "    global_max_length=global_max_length,\n",
    "    initial_rescale=1.0,\n",
    "    rescale_factor=0.8,\n",
    "    num_iter_adjust_rescale=20,\n",
    "    initial_alpha=0.01,\n",
    "    increase_factor_alpha=1.2,\n",
    "    num_iter_increase_alpha=20,\n",
    "    decrease_factor_alpha=0.8,\n",
    "    num_iter_decrease_alpha=20,\n",
    "    batch_size=2,\n",
    "    use_amp=True,\n",
    "    opt_level=\"O1\",\n",
    "    loss_scale=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack labels\n",
    "y = np.array([label1, label2, label3])\n",
    "\n",
    "tmp = list(y[0])\n",
    "tmp[-1] = 'F'\n",
    "y[0] = \"\".join(tmp)\n",
    "\n",
    "tmp = list(y[1])\n",
    "del tmp[-1]\n",
    "tmp[-1] = 'L'\n",
    "tmp[-2] = 'A'\n",
    "y[1] = \"\".join(tmp)\n",
    "\n",
    "tmp = list(y[2])\n",
    "del tmp[0]\n",
    "del tmp[5]\n",
    "y[2] = \"\".join(tmp)\n",
    "\n",
    "# Generate attack\n",
    "x_adv = asr_attack.generate(x[2:], y[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_transcriptions = speech_recognizer.predict(x_adv, batch_size=2, transcription_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

# MIT License
#
# Copyright (C) The Adversarial Robustness Toolbox (ART) Authors 2020
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
# documentation files (the "Software"), to deal in the Software without restriction, including without limitation the
# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit
# persons to whom the Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the
# Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE
# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""
Testing for the pe_malware_attack which implements the following white-box attacks related to PE malware crafting:
    1) Append based attacks (example paper link: https://arxiv.org/abs/1810.08280)
    2) Section insertion attacks (example paper link: https://arxiv.org/abs/2008.07125)
    3) Slack manipulation attacks (example paper link: https://arxiv.org/abs/1810.08280)
    4) DOS Header Attacks (example paper link: https://arxiv.org/abs/1901.03583)
"""

import copy
import numpy as np

from art.attacks.evasion.pe_malware_attack import MalwareGD
from tests.utils import TestBase


class TestMalwareAttack(TestBase):
    """
    A unittest class for testing the pe_malware_attack attack.
    """

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        x, y, size_of_files = cls.generate_synthetic_data()

        # make a copy of the original data.
        cls.original_data = copy.copy(x)
        cls.original_labels = copy.copy(y)
        cls.original_sizes = copy.copy(size_of_files)

        # setup classifier
        param_dic = {"maxlen": 2 ** 20, "input_dim": 257, "embedding_size": 8}
        model_weights = cls.make_dummy_model()
        cls.attack = MalwareGD(model_weights=model_weights, l_0=0, param_dic=param_dic)
        cls.x = x
        cls.y = y
        cls.size_of_files = size_of_files

    def test_no_perturbation(self):
        """
        Assert that with 0 perturbation the data is unmodified
        """
        print("running test_1")
        # First check: with no pertubation the malware of sufficient size, and benign files, should be unperturbed
        self.attack.l_0 = 0
        adv_x, adv_y, adv_sizes = self.attack.pull_out_valid_samples(self.x, self.y, self.size_of_files)

        # We should only have 3 files as the following cannot be converted to valid adv samples:
        #   2nd datapoint (file to large to support any modificaitons)
        #   5th datapoint (benign file)

        assert len(adv_x) == 3

        adv_x = self.attack.generate(adv_x, adv_y, adv_sizes)

        j = 0
        for i in range(len(self.original_data)):
            if i in [0, 2, 3]:
                assert np.array_equal(adv_x[j], self.original_data[i])
                j += 1
            else:
                assert np.array_equal(self.x[i], self.original_data[i])

    def test_append_attack(self):
        """
        Check append attack wih a given l0 budget
        """
        print("running test_append_attack")

        l0_budget = 1250
        self.attack.l_0 = l0_budget
        adv_x, adv_y, adv_sizes = self.attack.pull_out_valid_samples(self.x, self.y, self.size_of_files)

        # We should only have 2 files as the following cannot be converted to valid adv samples:
        #   2nd datapoint (file to large to support any modificaitons)
        #   4th datapoint (file to large to support append attacks)
        #   5th datapoint (benign file)

        assert len(adv_x) == 2

        adv_x = self.attack.generate(adv_x, adv_y, adv_sizes)

        j = 0
        for i, size in enumerate(self.original_sizes):
            if i in [0, 2]:
                assert np.array_equal(adv_x[j, :size], self.original_data[i, :size])
                assert not np.array_equal(
                    adv_x[j, size : size + l0_budget], self.original_data[i, size : size + l0_budget]
                )
                assert np.array_equal(adv_x[j, size + l0_budget :], self.original_data[i, size + l0_budget :])
                j += 1
            else:
                assert np.array_equal(self.x[i], self.original_data[i])

    def test_slack_attacks(self):
        """
        Testing modification of certain regions in the PE file
        """
        print("running test_slack_attacks")

        # Third check: Slack insertion attacks.
        l0_budget = 1250
        self.attack.l_0 = l0_budget
        batch_of_section_starts, batch_of_section_sizes = self.generate_synthetic_slack_regions(size=250)
        adv_x, adv_y, adv_sizes, batch_of_section_starts, batch_of_section_sizes = self.attack.pull_out_valid_samples(
            self.x,
            self.y,
            sample_sizes=self.size_of_files,
            perturb_starts=batch_of_section_starts,
            perturb_sizes=batch_of_section_sizes,
        )

        # We should only have 2 files as the following cannot be converted to valid adv samples:
        #   2nd datapoint (file to large to support any modificaitons)
        #   4th datapoint (attack requires appending 250 bytes to end of file which this datapoint cannot support)
        #   5th datapoint (benign file)
        assert len(adv_x) == 2

        adv_x = self.attack.generate(
            adv_x, adv_y, adv_sizes, perturb_sizes=batch_of_section_sizes, perturb_starts=batch_of_section_starts
        )

        j = 0
        for i, size in enumerate(self.original_sizes):
            if i in [0, 2]:
                slack_starts = batch_of_section_starts[j]
                slack_sizes = batch_of_section_sizes[j]
                begning_pos = 0
                total_pertubation = 0
                for slack_start, slack_size in zip(slack_starts, slack_sizes):
                    assert np.array_equal(
                        adv_x[j, begning_pos:slack_start], self.original_data[i, begning_pos:slack_start]
                    )

                    assert not np.array_equal(
                        adv_x[j, begning_pos : begning_pos + slack_start + slack_size],
                        self.original_data[i, begning_pos : begning_pos + slack_start + slack_size],
                    )
                    begning_pos = slack_start + slack_size
                    total_pertubation += slack_size

                # from the last slack region to end of file
                assert np.array_equal(adv_x[j, begning_pos:size], self.original_data[i, begning_pos:size])
                remaining_perturbation = l0_budget - total_pertubation
                assert remaining_perturbation == 250

                # append portion of the attack has been conducted
                assert not np.array_equal(
                    adv_x[j, size : size + remaining_perturbation],
                    self.original_data[i, size : size + remaining_perturbation],
                )

                # from end of append to end of datapoint
                assert np.array_equal(
                    adv_x[j, size + remaining_perturbation :], self.original_data[i, size + remaining_perturbation :]
                )

                j += 1
            else:
                assert np.array_equal(self.x[i], self.original_data[i])

    def test_large_append(self):
        """
        Testing with very large perturbation budgets
        """
        # Fourth check append large perturbation

        l0_budget = int(((2 ** 20) * 0.2))
        self.attack.l_0 = l0_budget
        adv_x, adv_y, adv_sizes = self.attack.pull_out_valid_samples(self.x, self.y, sample_sizes=self.size_of_files)
        # We should only have one datapoint that can support an append perturbation of this size
        assert len(adv_x) == 1

        adv_x = self.attack.generate(adv_x, adv_y, adv_sizes)

        j = 0
        for i, size in enumerate(self.original_sizes):
            if i == 0:
                assert np.array_equal(adv_x[j, :size], self.original_data[i, :size])
                assert not np.array_equal(
                    adv_x[j, size : size + l0_budget], self.original_data[i, size : size + l0_budget]
                )
                assert np.array_equal(adv_x[j, size + l0_budget :], self.original_data[i, size + l0_budget :])
            else:
                assert np.array_equal(self.x[i], self.original_data[i])

    def test_dos_header_attack(self):
        """
        Test the DOS header attack modifies the correct regions
        """
        # 5th check: DOS header attack

        l0_budget = 290
        self.attack.l_0 = l0_budget

        dos_starts, dos_sizes = self.attack.get_dos_locations(self.x)

        adv_x, adv_y, adv_sizes, batch_of_section_starts, batch_of_section_sizes = self.attack.pull_out_valid_samples(
            self.x, self.y, sample_sizes=self.size_of_files, perturb_starts=dos_starts, perturb_sizes=dos_sizes
        )

        # should have 3 files. Samples which are excluded are:
        #   2nd datapoint (file to large to support any modifications)
        #   5th datapoint (benign file)

        assert len(adv_x) == 3

        adv_x = self.attack.generate(
            adv_x, adv_y, adv_sizes, perturb_sizes=batch_of_section_sizes, perturb_starts=batch_of_section_starts
        )

        j = 0
        for i in range(len(self.original_sizes)):
            if i in [0, 2, 3]:
                assert np.array_equal(adv_x[j, 0:2], [77, 90])

                # we should have 58 bytes that were perturbed between the magic number and the pointer
                assert not np.array_equal(adv_x[j, 2 : int(0x3C)], self.original_data[i, 2 : int(0x3C)])

                # dummy pointer should be unchanged
                assert np.array_equal(adv_x[j, int(0x3C) : int(0x3C) + 4], [44, 1, 0, 0])

                # the remaining perturbation 290 - 58 = 232 is in the rest of the DOS header
                assert not np.array_equal(
                    adv_x[j, int(0x3C) + 4 : int(0x3C) + 4 + 232],
                    self.original_data[i, int(0x3C) + 4 : int(0x3C) + 4 + 232],
                )

                # rest of the file is unchanged
                assert np.array_equal(adv_x[j, int(0x3C) + 4 + 232 :], self.original_data[i, int(0x3C) + 4 + 232 :])
                j += 1

    def test_no_auto_append(self):
        """
        Verify behaviour when not spilling extra perturbation into an append attack
        """
        # 6th check: Do not automatically append extra perturbation
        l0_budget = 1250
        self.attack.l_0 = l0_budget

        batch_of_section_starts, batch_of_section_sizes = self.generate_synthetic_slack_regions(size=250)
        adv_x, adv_y, adv_sizes, batch_of_section_starts, batch_of_section_sizes = self.attack.pull_out_valid_samples(
            self.x,
            self.y,
            sample_sizes=self.size_of_files,
            perturb_starts=batch_of_section_starts,
            perturb_sizes=batch_of_section_sizes,
            automatically_append=False,
        )
        # should have 3 samples.
        # 2nd datapoint cannot support any modification
        # 5th is a benign sample
        assert len(adv_x) == 3

        adv_x = self.attack.generate(
            adv_x,
            adv_y,
            adv_sizes,
            automatically_append=False,
            perturb_sizes=batch_of_section_sizes,
            perturb_starts=batch_of_section_starts,
        )

        j = 0
        for i in range(len(self.original_sizes)):
            if i in [0, 2, 3]:
                slack_starts = batch_of_section_starts[j]
                slack_sizes = batch_of_section_sizes[j]
                begning_pos = 0
                total_pertubation = 0
                for slack_start, slack_size in zip(slack_starts, slack_sizes):
                    assert np.array_equal(
                        adv_x[j, begning_pos:slack_start], self.original_data[i, begning_pos:slack_start]
                    )

                    assert not np.array_equal(
                        adv_x[j, begning_pos : begning_pos + slack_start + slack_size],
                        self.original_data[i, begning_pos : begning_pos + slack_start + slack_size],
                    )
                    begning_pos = slack_start + slack_size
                    total_pertubation += slack_size
                # from end of final inserted perturbation to EOF.
                assert np.array_equal(adv_x[j, begning_pos:], self.original_data[i, begning_pos:])
                j += 1

    def test_do_not_check_for_valid(self):
        """
        No checking for valid data. Expect a mixed adversarial/normal data to be returned.
        """
        print("running test_append_attack")

        l0_budget = 1250
        self.attack.l_0 = l0_budget
        adv_x = self.x
        adv_y = self.y
        adv_sizes = self.size_of_files

        adv_x = self.attack.generate(adv_x, adv_y, adv_sizes, verify_input_data=False)
        assert len(adv_x) == 5

        # We expect 2 files to have been made adversarial the following cannot be converted to valid adv samples:
        #   2nd datapoint (file to large to support any modificaitons)
        #   4th datapoint (file to large to support append attacks)
        #   5th datapoint (benign file)
        for i, size in enumerate(self.original_sizes):
            if i in [0, 2]:
                assert np.array_equal(adv_x[i, :size], self.original_data[i, :size])
                assert not np.array_equal(
                    adv_x[i, size : size + l0_budget], self.original_data[i, size : size + l0_budget]
                )
                assert np.array_equal(adv_x[i, size + l0_budget :], self.original_data[i, size + l0_budget :])
            else:
                assert np.array_equal(adv_x[i], self.original_data[i])

    @staticmethod
    def generate_synthetic_data():
        """
        As real malware is hard to share, generate random data of the correct size
        """
        # generate dummy data
        padding_char = 256
        maxlen = 2 ** 20

        # batch of 5 datapoints
        synthetic_data = np.ones((5, maxlen), dtype=np.uint16) * padding_char

        size_of_original_files = [
            int(maxlen * 0.1),  # 1 sample significantly smaller than the maxlen
            int(maxlen * 1.5),  # 1 sample larger then maxlen
            int(maxlen * 0.95),  # 1 sample close to the maximum of the maxlen
            int(maxlen),  # 1 sample at the maxlen
            int(maxlen),
        ]  # 1 sample at the maxlen, this will be assigned a benign label and
        # should not be perturbed by the attack.

        # two class option, later change to binary when ART is generally updated.
        y = np.zeros((5, 1))
        y[0:4] = 1  # assign the first 4 datapoints to be labeled as malware

        # fill in with random numbers
        for i, size in enumerate(size_of_original_files):
            if size > maxlen:
                size = maxlen
            synthetic_data[i, 0:size] = np.random.randint(low=0, high=256, size=(1, size))

        # set the DOS header values:
        synthetic_data[:, 0:2] = [77, 90]
        synthetic_data[:, int(0x3C) : int(0x40)] = 0  # zero the pointer location
        synthetic_data[:, int(0x3C)] = 44  # put in a dummy pointer
        synthetic_data[:, int(0x3C) + 1] = 1  # put in a dummy pointer
        return synthetic_data, y, size_of_original_files

    @staticmethod
    def generate_synthetic_slack_regions(size):
        """
        Generate 4 slack regions per sample, each of size 250.
        """

        batch_of_slack_starts = []
        batch_of_slack_sizes = []

        for _ in range(5):
            size_of_slack = []
            start_of_slack = []
            start = 0
            for _ in range(4):
                start += 1000
                start_of_slack.append(start)
                size_of_slack.append(size)
            batch_of_slack_starts.append(start_of_slack)
            batch_of_slack_sizes.append(size_of_slack)
        return batch_of_slack_starts, batch_of_slack_sizes

    @staticmethod
    def make_dummy_model():
        """
        Create a random model for testing
        """
        model_weight_shapes = [
            (257, 8),
            (500, 8, 128),
            (128,),
            (500, 8, 128),
            (128,),
            (128, 128),
            (128,),
            (128, 1),
            (1,),
        ]
        model_weights = []

        for weights in model_weight_shapes:
            model_weights.append(np.random.normal(loc=0, scale=1.0, size=weights))

        return model_weights
